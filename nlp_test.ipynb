{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_ai_utils import *\n",
    "\n",
    "import keras\n",
    "keras.__version__\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses\n",
    "from keras.datasets import imdb\n",
    "from keras.utils.data_utils import pad_sequences \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "max_features = 10000 # number of words to consider as features\n",
    "maxlen = 500  # cut texts after this number of words (among top max_features most common words)\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(input_train.shape, 'train sequences')\n",
    "print(input_test.shape, 'test sequences')\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = pad_sequences(input_test, maxlen=maxlen)\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('y shape:', y_train.shape)\n",
    "print(y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "experimental_input_train = input_train\n",
    "experimental_y_train = y_train\n",
    "\n",
    "model2 = Sequential(usage=Usage.logisticRegression)\n",
    "model2.add_layer(Embedding(input_dim=max_features, output_dim=2, seq_length=maxlen))\n",
    "model2.add_layer(RNN(in_features=2, hidden_features=4, architecture=\"many_to_one\"))\n",
    "model2.add_layer(Dense(in_features=4, out_features=1, activation=\"sigmoid\"))\n",
    "\n",
    "embedding_w = model2.layers[0].kernel\n",
    "rnn_w = [model2.layers[1].Wxa, model2.layers[1].Waa, model2.layers[1].ba]\n",
    "dense_w = [model2.layers[2].kernel, model2.layers[2].biases]\n",
    "# learning_rate schedulers\n",
    "# lr = Warmup(target_lr=0.1, warm_steps=20)\n",
    "#lr = CosineDecay(initial_lr=0.1, alpha=0.001, warmup=False, warmup_steps=100, hold_steps=200)\n",
    "# lr = ExponentialDecay(initial_lr=0.01, decay_rate=0.2, warmup=True, warmup_steps=5, hold_steps=5)\n",
    "\n",
    "model2.compile(loss_fn=Loss(\"l2\"), optimizer=RMSprop(beta=0.9, lr=0.01))\n",
    "print(\"Initial for dense\", model2.layers[-1].kernel)\n",
    "print(\"Initial for rnn\", model2.layers[1].Wxa)\n",
    "print(\"Initial for rnn\", model2.layers[1].Waa)\n",
    "print(\"Initial for embedding\", model2.layers[0].kernel[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "model1 = models.Sequential()\n",
    "model1.add(layers.Embedding(max_features, 2))\n",
    "model1.add(layers.SimpleRNN(4, activation=\"tanh\"))\n",
    "model1.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.layers[0].set_weights([embedding_w])\n",
    "model1.layers[1].set_weights(rnn_w)\n",
    "model1.layers[2].set_weights(dense_w)\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model1.compile(optimizer=optimizer, loss='mse', metrics=['acc'])\n",
    "\n",
    "embedding_weights = model1.layers[0].get_weights()\n",
    "dense_weights = model1.layers[2].get_weights()\n",
    "rnn_weights = model1.layers[1].get_weights()\n",
    "print(\"Initial for dense \", model1.layers[2].get_weights()[0])\n",
    "print(\"Initial for rnn \", model1.layers[1].get_weights()[0])\n",
    "print(\"Initial for rnn \", model1.layers[1].get_weights()[1])\n",
    "print(\"Initial for embedding \", model1.layers[0].get_weights()[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.train(input_train, y_train, nepochs=10, batch_size=128)\n",
    "print(\"After for dense\", model2.layers[-1].kernel)\n",
    "print(\"After for rnn\", model2.layers[1].Wxa)\n",
    "print(\"After for embedding\", model2.layers[0].kernel[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model1.fit(input_train, y_train, epochs=10, batch_size=128)\n",
    "print(\"Initial for dense \", model1.layers[2].get_weights()[0])\n",
    "print(\"Initial for rnn \", model1.layers[1].get_weights()[0])\n",
    "print(\"Initial for rnn \", model1.layers[1].get_weights()[1])\n",
    "print(\"Initial for embedding \", model1.layers[0].get_weights()[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = model2.evaluate(input_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model1.predict(input_test)\n",
    "#print((y_pred < 0.3).sum() + (y_pred > 0.7).sum())\n",
    "#y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "#print(y_pred.sum() / len(y_pred))\n",
    "#print(y_test.sum() / len(y_test))\n",
    "Accuracy(y_pred, y_test)\n",
    "ConfusionMatrix(y_pred, y_test, classes=[0, 1], usage=Usage.logisticRegression)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
